---
title: "The AAA Algorithm"
categories:
  - Algorithms
date:   2025-01-23 17:59:00 +0100
mathjax: true
tags:
  - Least Squares
toc: true
# classes: wide
excerpt: ""
header: 
  overlay_image: assets/aaa-algo/images/splash_image.png
  overlay_filter: 0.2
---

# Motivation

Taylor series approximate functions with polynomials but there are many applications when the underlying data could be better described with a rational function. The "adaptive Antoulasâ€“
Anderson" algorithm is a new method for fitting data with a rational function.

# Rational functions

A rational function of two polynomials takes the form

$$
f(x) = {a_0+a_1x+a_2x^2+\cdots+a_mx^m \over 1+b_1x+b_2x^2+\cdots + b_mx^m}.
$$

Note how the denominator coefficients are normalised so that the bias is 1.

## Least Squares

Suppose we wanted to fit a rational function approximation to $$\log(1+x)$$ natural logarithm over the interval (0, 1].<sup>[1](#footnote1)</sup>
We can sample points at a spacing of 0.1, say, and create 50 pairs of the form $$(x_i, \log(1 + x_i))$$.

The rational function is clearly a non-linear function of the coefficient vector $$[a_0, a_1, \ldots, a_m, b_1, \ldots, b_m]$$.
But with some algebra we can express the coefficients as the solution of an ordinary least squares problem.

$$
y_i = a_0 + a_1x_i + a_2x_i^2 + \cdots + a_mx_i^m - (b_1x_i + b_2x_i^2 + \cdots + b_mx_i^m) y_i
$$

In matrix notation this is
$$
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N\\
\end{bmatrix} =
\begin{bmatrix}
1 & x_1 & \cdots & x_1^m & y_1x_1 & \cdots & y_1x_1^m\\
1 & x_2 & \cdots & x_2^m & y_2x_2 & \cdots & y_2x_2^m\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
1 & x_N & \cdots & x_N^m & y_Nx_N & \cdots & y_Nx_N^m\\
\end{bmatrix}
\begin{bmatrix}
a_0 \\
a_1 \\
\vdots \\
a_m\\
b_1\\
\vdots \\
b_m\\
\end{bmatrix}
$$

This is an OLS with $$N$$ equations and $$2m-1$$ parameters.
The first $$m$$ columns are called a _Vandermonde matrix_ of degree $$m$$.
Notice that the remaining columns are _nearly_ Vandermonde except for the missing first column and the multiplation by $$y_i$$.

Figure 1 shows rational functions approximations using OLS for various degrees $$m$$ compared with the value $$\log(1+x)$$.

<figure class>
    <a href="/assets/aaa-algorithm/images/ols_logarithm.png"><img src="/assets/aaa-algorithm/images/ols_logarithm.png"></a>
    <figcaption>Figure 1: Rational function approximations with degree 1 and 2 of log1p over the interval [0,1].</figcaption>
</figure>

The maximum absolute error in the interval [0,1] for the degree 1 rational function is 1.15e-3.
With a degree 2 polynomial, the error drops to 3e-6.

The following code can be used to solve for the parameter vector

{% highlight python %}
import numpy as np
from typing import Callable

def rational_function_ols(
  f: Callable,
  z: np.ndarray,
  m: int,
) -> tuple[np.ndarray, np.ndarray]:

    y = f(z)  # (N,)
    A_1 = np.vander(z, m + 1, increasing=True)  # (M, m+1)
    A_2 = -y[:, None] * A_1[:, 1:]

    # f(x_k) = a_0 + a_1 * x_k + ... + a_m * x_k ** m - 
    #      b_1 * f(x_k) * x_k - ... - b_m * f(x_k) * x_k ** m
    A = np.concat([A_1, A_2], axis=1)  # (N, 2*m + 1)

    theta, _, _, _ = np.linalg.lstsq(A, y)

    a = theta[: m + 1]
    b = theta[m + 1 :]
    return a, b 
{% endhighlight %}

But what happens if we apply this least squares method to a less well behaved function like the gamma fuction shown in Figure 2?

<figure class>
    <a href="/assets/aaa-algorithm/images/gamma_function.png"><img src="/assets/aaa-algorithm/images/gamma_function.png"></a>
    <figcaption>Figure 2: The gamma function on the interval (-3,3]. Note the asymptotes at negative integers.</figcaption>
</figure>

The following Figures show rational functions of varying degrees fit to the gamma function

<figure class="half">
    <a href="/assets/aaa-algorithm/images/ols_gamma_degree_01.png"><img src="/assets/aaa-algorithm/images/ols_gamma_degree_01.png"></a>
    <a href="/assets/aaa-algorithm/images/ols_gamma_degree_02.png"><img src="/assets/aaa-algorithm/images/ols_gamma_degree_02.png"></a>
    <figcaption>Figure 3: Degree 1 and 2 OLS fits to the gamma function are quite poor</figcaption>
</figure>

<figure class="half">
    <a href="/assets/aaa-algorithm/images/ols_gamma_degree_08.png"><img src="/assets/aaa-algorithm/images/ols_gamma_degree_08.png"></a>
    <a href="/assets/aaa-algorithm/images/ols_gamma_degree_09.png"><img src="/assets/aaa-algorithm/images/ols_gamma_degree_09.png"></a>
    <figcaption>Figure 4: Degree 8 and 9 OLS fits to the gamma function are quite poor</figcaption>
</figure>

Figure 3 shows how poor low degree polynomials fit in this scenario.
Not until degree 9 do we get a decent fit to the gamma fucntion.
But interestingly, if we continue with higher degree polynomials the approximation can actually degrade due to numerical issues as shown in Figure 5.

<figure class="half">
    <a href="/assets/aaa-algorithm/images/ols_gamma_degree_10.png"><img src="/assets/aaa-algorithm/images/ols_gamma_degree_10.png"></a>
    <a href="/assets/aaa-algorithm/images/ols_gamma_degree_11.png"><img src="/assets/aaa-algorithm/images/ols_gamma_degree_11.png"></a>
    <figcaption>Figure 5: Degree 10 and 11 OLS fits to the gamma function are worse than the degree 9 fit.</figcaption>
</figure>

Is there a better way to fit rational functions?

### AAA

A paper from 2017 introduces the AAA algorithm for rational function approximation.
Rather than represent the rational function as an $$m$$ degree polynomial divided by another $$m$$ degree polynomial as we did in the previous section, the authors propose the curious form

$$
f(x) = {\sum_{k=1}^m {w_ky_k \over x - x_k } \over \sum_{k=1}^m {w_k \over x - x_k } }
$$

where $$w_k$$ are the coefficients to estimate.
The algorithm is iterative and first chooses the by dividing the training data into support points and non-support points.

First choose the point with the largest abs differnt from the mean of y.
Then try to fit a single w using the other N-1 points and solving a least squares problem.
Note how the function will interpolate the support point automatically because of the removable singularity.

Form the least squares problem subject to unit ball constraint.
Unit ball constraint is because we can arbitrarily scale w (maybe we should do this for the OLS problem?)
Solution to the problem is just the minimum singular value.
Then find the point that has the largest max abs error and add it to the support set and rerun the inner loop with N-2 points

If the set of support points is $$\tilde{x}_1, \tilde{x}_2, \ldots, \tilde{x}_m$$ and the non-support points $$x_1, x_2, \ldots, x_{N-m}$$, the core of the al

At each iteration of the algorithm
This can be rewritten as

$$
y_k\sum_{k=1}^m {w_k \over x - x_k } =\sum_{k=1}^m {w_ky_k \over x - x_k }
$$

## Conclusion

## Footnotes

<a name="footnote1">1</a>: We can use properties of logarithms to perform _argument reduction_ so that we always have a logarithm of the form $$\log(1 + x)$$ with $$0\le x\le 1$$

## References

1. [Scipy 1.15.0 release notes](https://docs.scipy.org/doc/scipy/release/1.15.0-notes.html)
3. [Original paper](https://arxiv.org/abs/1612.00337)
