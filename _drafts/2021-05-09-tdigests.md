---
title: "Approximate Percentiles with _t_-digests"
categories:
  - Mathematics
date:   2021-05-21 22:19:00 +0100
mathjax: true
tags:
  - Algorithms
toc: true
# classes: wide
excerpt: ""
header: 
  overlay_image: assets/images/shakespeare-zipf-param-surface-splash.png
  overlay_filter: 0.2
---

Consider the problem of computing summary statistics on a large data set. The data is so large that loading it all into memory is completely infeasible. But the data has the nice property that it has been sharded across many files, each of which do fit in memory. 

It is not difficult to work out an algorithm for computing some statistics across all of the data. For example, the maximum is one such statistic. The simplest method is to initialise the maximum to negative infinity and then read a single file into memory and compute its maximum. If it is greater than the current maximum, update it. Repeat until all the files have been read.

But suppose instead you are interested in knowing the 95th percentile of the data. In general, knowing the 95th percentile of the data in each shard of the data is insufficient to know the 95th percentile of the data globally. The _t_-digest is a probablistic data structure that allows accurate estimation of arbitrary percentiles of the _global_ data without keeping all of the data in memory.

# Clustering in 1D
The _t_-digest relies on compressing univariate data into a small number of clusters while still being representative of the overall distribution (at least approximately). A _cluster_ is simply a collection of points on a number a line. A simple summary of a 1D cluster is provided by its center of mass (also called its centroid) and the number of points it contains (also called its weight). This is encapsulated in the following Python classes:

{% highlight python %}
from typing import List

class Cluster:
  def __init__(self, values: List[float]):
    self._values = values

  def summarise(self) -> ClusterSummary:
    n_points = len(self._values)
    center_of_mass = sum(self._values) / n_points
    
    return ClusterSummary(center_of_mass, n_points)

class ClusterSummary:
  def __init__(self, center_of_mass: float, n_points: int):
    self._center_of_mass = center_of_mass
    self._n_points = n_points

  @property
  def centroid(self):
    return self._center_of_mass

  @property
  def weight(self):
    return self._n_points

{% endhighlight %}

To understand how the compression is done, it is easiest to consider the case when all of the data is available in memory. Below are three possible 1d clusterings of 25 data points. 

<figure>
    <a href="/assets/images/t-digest/strongly-ordered-clusters.png"><img src="/assets/images/t-digest/strongly-ordered-clusters.png"></a>
    <figcaption>Figure 1</figcaption>
</figure>

The clusters in Figure 1 have a property that the authors of the [_t_-digest paper](https://arxiv.org/abs/1902.04023) refer to as _strongly ordered_. A collection of clusters is called strongly ordered if a cluster with a smaller center of mass than another is guaranteed to have all of its points be less than that of the cluster with the larger center of mass. 

This strong ordering property can be observed visually in Figure 1. Note that all of the data points belonging to the blue cluster are less than all of the data points belonging to the green cluster, the points in the green cluster are all less than those in the yellow cluster and so on. In contrast, the clusters in Figure 2 do not exhibit strong ordering. Even though the centroid of the yellow cluster is greater than that of the green cluster, there are points belonging to the yellow cluster that are less than some of the points in the green cluster.

<figure>
    <a href="/assets/images/t-digest/weakly-ordered-cluster.png"><img src="/assets/images/t-digest/weakly-ordered-cluster.png"></a>
    <figcaption>Figure 2</figcaption>
</figure>

One of the main contributions of the paper is a systematic way of choosing the boundaries of the clusters. The _t_-digest uses what the authors call a _scale function_ which maps a percentile to a real number, which they denote by $$k$$. This function is used to define a criterion for how many points should be allowed in each cluster (more on this later). 

When a cluster is created, the percentile of the first data point in the cluster is mapped to a $$k$$ value and saved. Points are continually added to the cluster (in sorted order) until enough points are added such that the $$k$$ value has increased by 1. At this point, the cluster is considered complete and the centroid as well as its weight are stored and a new cluster is started.

The cluster building process is shown in the animation in Figure 3. The crosses denote cluster centroids. Note how there are fewer points in the clusters at the extreme quantiles where the scale function is steeper.

<figure>
    <a href="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"><img src="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"></a>
    <figcaption>Figure 3</figcaption>
</figure>

Below is an implementation of this algorithm in Python.

{% highlight python %}
import numpy as np

TAU = 2 * np.pi

def scale_fn(q, delta):
  return delta / TAU * np.arcsin(2 * q - 1)

def cluster_points(points):
  sorted_points = np.sort(points)
  data_clusters = [[]]
  k_lower = scale_fn(0, delta)
  percentile_increment = 1 / n_points
  centroids = []
  for j, pt in enumerate(sorted_points):
      percentile = (j + 1) * percentile_increment
      k_upper = scale_fn(percentile, delta)

      if k_upper - k_lower < 1:
          data_clusters[-1].append(pt)
      else:
          centroids.append(np.mean(data_clusters[-1]))
          data_clusters.append([pt])
          k_lower = k_upper

  return data_clusters
{% endhighlight %}

## Scale Functions
At first glance, the choice of scale function seems complicated and arbitrary. 

The first property of a valid scale function is that it must be defined over the domain $$[0,1]$$. This ensures every percentile is mapped to a $$k$$ value. However, the main property of the scale function is that it is monotone increasing. Without this property, it is possible that the change in $$k$$ would never exceed 1, leading to no clustering at all.

Perhaps the more fundamental object than the scale function is its derivative. The slope of the scale function determines how fast the $$k$$ value increases with each percentile. This effectively sets the number of points that can be added to a cluster before it is complete. The larger the slope, the fewer points will be allowed in the cluster. The smaller the slope, the more points will be included before a new cluster is started. 

To make this notion more precise, consider the linearisation of the scale function $$k(q)$$

$$\Delta k \approx \frac{\mathrm{d}k}{\mathrm{d}q}\Delta q$$

The condition for finishing a cluster, $$\Delta k = 1$$, can be rewritten as $$\frac{\mathrm{d}q}{\mathrm{d}k} = \Delta q$$. Since $$\Delta q$$ is just the number of points in the cluster divided by the total number of data points, it can be said that the cluster sizes are proportional to $$\frac{\mathrm{d}q}{\mathrm{d}k}$$ (at least approximately). For the inverse sine scale function in the previous section, this derivative is $$\sqrt{q(1-q)}$$. This means the extreme percentiles (near 0 or 1) will be approximated by smaller, more accurate clusters (see Figure 3)

The paper goes on to suggest four scale functions with various desirable properties

$$
\begin{align*}
k_0(q) &= \frac{\delta}{2} q\\
k_1(q) &= \frac{\delta}{2\pi} \sin^{-1}(2q-1)\\
k_2(q) &= \frac{\delta}{4 \log(n/\delta) + 24} \log\left(\frac{q}{1-q}\right)\\
k_3(q) &= \frac{\delta}{4 \log(n/\delta) + 21} \cdot \left\{
     \begin{array}{lr}
       \log(2q) & q \le 0.5\\
       -2\log(2(1-q)) & q > 0.5\\
     \end{array}
   \right.
\end{align*}
$$

Each scale function has a scale parameter $$\delta$$ which determines the number of cluster centroids used to approximate the CDF. Larger values of $$\delta$$ equate to steeper scale functions. Since a steep scale function means few points will be allowed in a cluster, this translates to having many clusters. A small $$\delta$$ of course means the CDF will be approximated with fewer clusters.

<figure>
    <a href="/assets/images/t-digest/scale-functions.png"><img src="/assets/images/t-digest/scale-functions.png"></a>
    <figcaption>Figure 4</figcaption>
</figure>

# Defining the _t_-digest
The _t_-digest data structure is just a scale function along with a collection of cluster summaries (sorted by centroid). In order to be useful, the data structure also needs to support the following two operations: 

1. Combine multiple existing _t_-digests into a single _t_-digest
2. Compute arbitrary percentile estimates

The following sections discuss each of these in more detail.

## Merging digests
The preceeding sections outline the case when the data fits into memory (with some room left over to sort the data). In the scenario where each data file fits into memory, a single _t_-digest can be computed per file. To compute a single _t_-digest summarising the entire dataset, we can create a list of all the cluster summaries from each _t_-digest, sorted by their centroids. 

The cluster summaries can then merged in a greedy fashion by determining if the weight of the merged cluster leads to an increase in the $$k$$ value less than 1. If so, the two clusters can be replaced with a single merged one using the following code

{% highlight python %}
def merge_cluster_summaries(c1, c2):

  w1 = c1.weight / (c1.weight + c2.weight)
  w2 = c2.weight / (c1.weight + c2.weight)

  new_centroid = w1 * c1.centroid + w2 * c2.centroid
  new_weight = c1.weight + c2.weight

  return ClusterSummary(new_centroid, new_weight)
{% endhighlight %}

For the most part, this algorithm is the same as the in-memory case. The only difference is that instead of adding points to a cluster, clusters are added to larger clusters. 

Recall that the cluster summaries created from in-memory data were guaranteed to be strongly ordered, by construction. That is, a cluster with a centroid to the left of another cluster's centroid must have every data point less than that of the other cluster. Unfortunately when merging cluster summaries, this property is no longer guaranteed to hold.  The consequence of this is error in the estimation of the percentiles.

## Percentile Estimation
With the _t_-digest defined as well as methods for combining existing digests, the natural question is, "how can a _t_-digeset be used to estimate an arbitrary percentile". In keeping only the centroids, along with the number of points, it is clear that there is no way to get an exact answer to this question.

The assumption made for the purposes of computing percentiles from a _t_-digest is that the centroid is not just the mean of the data in the cluster, but the _median_ as well. Stated another way, the assumption is that if there are $$N$$ data points in a cluster, $$N/2$$ will be to the left of the centroid and $$N/2$$ will be to the right. Of course, this assumption need not hold and it is easy to find examples where this assumption is clearly false. In practice, the $$\delta$$ parameter can be tuned to create finer grained clusters so as to avoid badly skewed clusters. 

Given a percentile to compute, the sorted centroids are traversed in sorted order. As the list is iterated, half of the weight of the current cluster is added to the accumulated weight of all of the preceding clusters (due to the median assumption). Dividing this quantity by the total weight of all the clusters gives an approximate percentile. When the desired percentile is first exceeded, a linear interpolation is performed

# make a gif showing 2d plot of percentile vs value with cluster centroids

<figure>
    <a href="/assets/images/t-digest/approximate-cdf.png"><img src="/assets/images/t-digest/approximate-cdf.png"></a>
    <figcaption>Figure 5 </figcaption>
</figure>

{% highlight python %}
def cdf(td, x):
    total_weight = sum(c.weight for c in td.clusters)

    prev_percentile = 0
    percentile = 0
    prev_weight = 0
    for i, cluster in enumerate(td.clusters):
      if cluster.weight == 1:
        percentile += 1 / total_weight
        prev_weight = 0
      else:
        percentile += (cluster.weight / 2 + prev_weight / 2) / total_weight
        prev_weight = cluster.weight

      if x < cluster.centroid:
        if i == 0:
          return 0.
        delta_x = abs(cluster.centroid - prev_cluster.centroid)
        delta_y = abs(percentile - prev_percentile)
        m = delta_y / delta_x

        return prev_percentile + m * abs(x - prev_cluster.centroid)

      prev_percentile = percentile
      prev_cluster = cluster

    return 1.
{% endhighlight %}

# Conclusion
The _t_-digest is essentially a compressed representation of the data's cumulative distribution function (CDF). 

## Footnotes
<a name="footnote1">1</a>: A function is convex if $$f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$$.  

## References
1. [t-digest Paper on Arxiv](https://arxiv.org/abs/1902.04023)
2. [Intuitive Explanation of t-digest](https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest)
