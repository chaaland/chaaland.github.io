---
title: "Approximate Percentiles with _t_-digests"
categories:
  - Algorithms
date:   2021-05-21 22:19:00 +0100
mathjax: true
tags:
  - Approximation
  - Performance
toc: true
# classes: wide
excerpt: ""
# header: 
#   overlay_image: assets/t-digests/images/.png
#   overlay_filter: 0.2
---

Consider the problem of computing summary statistics on a large data set. The data is so large that loading it all into memory is completely infeasible. But the data has the nice property that it has been sharded across many files, each of which _do_ fit in memory.

It is not difficult to work out an algorithm for computing some statistics across all of the data. For example, the minimum, maximum, mean, and even the variance are such examples.

Suppose instead you are interested in knowing the 95th percentile of the data. In general, knowing the 95th percentile of the data in each shard of the data is insufficient to know the 95th percentile of the data globally. The _t_-digest is a probablistic data structure that allows accurate estimation of arbitrary percentiles of the _global_ data without keeping all of the data in memory.

# Clustering in 1D

The _t_-digest relies on compressing univariate data into a small number of clusters while still being representative of the overall distribution (at least approximately). A _cluster_ is simply a collection of points on a number a line. A simple summary of a 1D cluster is provided by its center of mass (also called its centroid) and the number of points it contains (also called its weight). This is encapsulated in the following Python classes:

{% highlight python %}
from typing import List

class Cluster:
  def __init__(self, values: List[float]):
    self._values = values

  def summarise(self) -> ClusterSummary:
    n_points = len(self._values)
    center_of_mass = sum(self._values) / n_points

    return ClusterSummary(center_of_mass, n_points)

class ClusterSummary:
  def __init__(self, center_of_mass: float, n_points: int):
    self._center_of_mass = center_of_mass
    self._n_points = n_points

  @property
  def centroid(self):
    return self._center_of_mass

  @property
  def weight(self):
    return self._n_points

{% endhighlight %}

To understand how the compression is done, it is easiest to consider the case when all of the data is available in memory. Below are three possible 1d clusterings of 25 data points.

<figure>
    <a href="/assets/t-digest/images/strongly-ordered-clusters.png"><img src="/assets/t-digest/images/strongly-ordered-clusters.png"></a>
    <figcaption>Figure 1</figcaption>
</figure>

The clusters in Figure 1 have a property that the authors of the [_t_-digest paper](https://arxiv.org/abs/1902.04023) refer to as _strongly ordered_. A collection of clusters is called strongly ordered if a cluster with a smaller center of mass than another is guaranteed to have all of its points be less than that of the cluster with the larger center of mass.

This strong ordering property can be observed visually in Figure 1. Note that all of the data points belonging to the blue cluster are less than all of the data points belonging to the green cluster, the points in the green cluster are all less than those in the yellow cluster and so on. In contrast, the clusters in Figure 2 do not exhibit strong ordering. Even though the centroid of the yellow cluster is greater than that of the green cluster, there are points belonging to the yellow cluster that are less than some of the points in the green cluster.

<figure>
    <a href="/assets/t-digest/images/weakly-ordered-cluster.png"><img src="/assets/t-digest/images/weakly-ordered-cluster.png"></a>
    <figcaption>Figure 2</figcaption>
</figure>

One of the main contributions of the paper is a systematic way of choosing the boundaries of the clusters. The _t_-digest uses what the authors call a _scale function_ which maps a percentile to a real number, which is denoted by $$k$$. This function is used to define a criterion for how many points should be allowed in each cluster (more on this later).

When a cluster is created, the percentile of the first data point in the cluster is mapped to a $$k$$ value and saved. Points are continually added to the cluster (in sorted order) until enough points are added such that the $$k$$ value has increased by 1. At this point, the cluster is considered complete and the centroid as well as its weight are stored and a new cluster is started.

The cluster building process is shown in the animation in Figure 3. The crosses denote cluster centroids. Note how there are fewer points in the clusters at the extreme quantiles where the scale function is steeper.

<figure>
    <a href="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"><img src="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"></a>
    <figcaption>Figure 3</figcaption>
</figure>

Below is an implementation of this algorithm in Python.

{% highlight python %}
import numpy as np

TAU = 2 * np.pi

def scale_fn(q, delta):
  return delta / TAU *np.arcsin(2* q - 1)

def cluster_points(points):
  sorted_points = np.sort(points)
  data_clusters = [[]]
  k_lower = scale_fn(0, delta)
  percentile_increment = 1 / n_points
  centroids = []
  for j, pt in enumerate(sorted_points):
      percentile = (j + 1) * percentile_increment
      k_upper = scale_fn(percentile, delta)

      if k_upper - k_lower < 1:
          data_clusters[-1].append(pt)
      else:
          centroids.append(np.mean(data_clusters[-1]))
          data_clusters.append([pt])
          k_lower = k_upper

  return data_clusters
{% endhighlight %}

## Scale Functions

At first glance, the choice of scale function seems complicated and arbitrary.

The first property of a valid scale function is that it must be defined over the domain $$[0,1]$$. This ensures every percentile is mapped to a $$k$$ value. However, the main property of the scale function is that it is monotone increasing. Without this property, it is possible that the change in $$k$$ would never exceed 1, leading to no clustering at all.

Perhaps the more fundamental object than the scale function is its derivative. The slope of the scale function determines how fast the $$k$$ value increases with each percentile. This effectively sets the number of points that can be added to a cluster before it is complete. The larger the slope, the fewer points will be allowed in the cluster. The smaller the slope, the more points will be included before a new cluster is started.

To make this notion more precise, consider the linearisation of the scale function $$k(q)$$

$$\Delta k \approx \frac{\mathrm{d}k}{\mathrm{d}q}\Delta q$$

Denoting the total number of points as $$N$$, the number of points in the $$i$$th cluster as $$n_{c_i}$$, and inserting the condition for finishing a cluster ($$\Delta k \approx 1$$), this becomes

$$
\begin{align*}
\frac{\mathrm{d}q}{\mathrm{d}k} &\approx \Delta q\\
&\approx n_{c_i} / N\\
&\propto n_{c_i}
\end{align*}
$$

it is evident that the cluster sizes, $$n_{c_i}$$ are proportional to $$\frac{\mathrm{d}q}{\mathrm{d}k}$$ (at least approximately).

For the  arcsine scale function in the previous section, this derivative is $$\sqrt{q(1-q)}$$. This means the extreme percentiles (near 0 or 1) will be approximated by smaller, more accurate clusters (see Figure 3).

The paper suggests four scale functions with various desirable properties

$$
\begin{align*}
k_0(q) &= \frac{\delta}{2} q\\
k_1(q) &= \frac{\delta}{2\pi} \sin^{-1}(2q-1)\\
k_2(q) &= \frac{\delta}{4 \log(n/\delta) + 24} \log\left(\frac{q}{1-q}\right)\\
k_3(q) &= \frac{\delta}{4 \log(n/\delta) + 21} \cdot \left\{
     \begin{array}{lr}
       \log(2q) & q \le 0.5\\
       -2\log(2(1-q)) & q > 0.5\\
     \end{array}
   \right.
\end{align*}
$$

Each scale function has a scale parameter $$\delta$$ which determines the number of cluster centroids used in the compression of the CDF. Larger values of $$\delta$$ equate to steeper scale functions. Since a steep scale function means fewer points per cluster, this translates to having many clusters. Conversely, a small $$\delta$$ means the CDF will be approximated with fewer clusters.

<figure>
    <a href="/assets/t-digest/images/scale-functions.png"><img src="/assets/t-digest/images/scale-functions.png"></a>
    <figcaption>Figure 4</figcaption>
</figure>

# Defining the _t_-digest

The _t_-digest data structure is just a scale function along with a collection of cluster summaries (sorted by centroid). In order to be useful, the data structure also needs to support the following two operations:

1. Combine multiple existing _t_-digests into a single _t_-digest
2. Compute arbitrary percentile estimates

The following sections discuss each of these in more detail.

## Merging digests

The preceeding sections outline the case when the data fits in main memory (with some room left over to sort the data). In the scenario where each data file fits in memory, a single _t_-digest can be computed per file. To compute a single _t_-digest summarising the entire dataset, we can create a list of all the cluster summaries from each _t_-digest, sorted by their centroids.

The cluster summaries can then be merged in a greedy fashion by determining if the weight of the merged cluster leads to an increase in the $$k$$ value less than 1. If so, the two clusters can be replaced with a single merged one using the following code

{% highlight python %}
def merge_cluster_summaries(c1, c2):

  w1 = c1.weight / (c1.weight + c2.weight)
  w2 = c2.weight / (c1.weight + c2.weight)

  new_centroid = w1 *c1.centroid + w2* c2.centroid
  new_weight = c1.weight + c2.weight

  return ClusterSummary(new_centroid, new_weight)
{% endhighlight %}

For the most part, this algorithm is the same as the in-memory case. In the in-memory case, individaul points were added to a cluster and in the merging of _t_-digests, clusters are added to larger clusters. Of course this is a distinction without a difference as individual points can be viewed as clusters with weight 1 and centroid equal to the point's value.

One thing that is materially different from the in-memory case is the strong ordering property. Unfortunately, when merging cluster summaries, this property is no longer guaranteed to hold. The consequence of this is error in the estimation of the percentiles.

## Percentile Estimation

The natural question is, "how can a _t_-digest be used to estimate an arbitrary percentile?". By keeping only the centroids along with the number of points, the best we can do is to get an approximate answer to this question.

The assumption made for the purpose of computing percentiles from a _t_-digest is that the centroid is not just the mean of the data in the cluster, but the _median_ as well. Stated another way, if there are $$n_{c_i}$$ data points in the $$i$$th cluster, $$n_{c_i}/2$$ will be to the left of the centroid and $$n_{c_i}/2$$ will be to the right. Of course, this assumption need not hold and it is easy to find examples where it is false. Without more assumptions on the distribution of the data, this is the best we can do.

The approximate percentile for the $$i$$th cluster  (sorted ascending by centroid) is given by the following recurrence

$$ {\tt cdf}[i] = \frac{-{\tt w}[i] / 2 + \sum_{k=1}^i {\tt w}[k]}{\sum_{k=1}^N {\tt w}[k]} $$

where $${\tt w}[i]$$ gives the weight of the $$i$$th cluster and $$N$$ is the number of clusters.

To estimate the $$p$$th percentile of data, first the largest $$i$$ such that $${\tt cdf}[i] \le p$$ is found (using binary search for example). Then a linear interpolation is performed between $${\tt centroid}[i]$$ and $${\tt centroid}[i+1]$$ to give the approximate percentile. This process is depicted using a linear search in Figure 5.

<figure>
    <a href="/assets/gifs/t-digest/percentile-estimation.gif"><img src="/assets/gifs/t-digest/percentile-estimation.gif"></a>
    <figcaption>Figure 5</figcaption>
</figure>

# Conclusion

The _t_-digest is essentially a compressed representation of the data's cumulative distribution function (CDF). By choosing an appropriate scale parameter, the level of compression can be tuned to use the desired amount of memory.

## References

1. [t-digest Paper on Arxiv](https://arxiv.org/abs/1902.04023)
2. [Intuitive Explanation of t-digest](https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest)
