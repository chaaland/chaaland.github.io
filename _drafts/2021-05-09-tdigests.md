---
title: "Approximate Percentiles with _t_-digests"
categories:
  - Mathematics
date:   2021-05-21 22:19:00 +0100
mathjax: true
tags:
  - Algorithms
toc: true
# classes: wide
excerpt: ""
header: 
  overlay_image: assets/images/shakespeare-zipf-param-surface-splash.png
  overlay_filter: 0.2
---

Consider the problem of computing summary statistics on a large data set. The data is so large that loading it all into memory is completely infeasible. But the data has the nice property that it has been sharded across many files, each of which do fit in memory. 

It is not difficult to work out an algorithm for computing some statistics across all of the data. For example, the maximum is one such statistic. The simplest method is to initialise the maximum to negative infinity and then read a single file into memory and compute its maximum. If it is greater than the current maximum, update it. Repeat until all the files have been read.

But suppose instead you are interested in knowing the 95th percentile of the data. In general, knowing the 95th percentile of the data in each shard of the data is insufficient to know the 95th percentile of the data globally. The _t_-digest is a probablistic data structure that allows accurate estimation of arbitrary percentiles of the _global_ data without keeping all of the data in memory.

# Clustering in 1D
The _t_-digest relies on compressing univariate data into a small number of clusters while still being representative of the overall distribution (at least approximately). A _cluster_ is simply a collection of points on a number a line. A simple summary of a 1D cluster is provided by its center of mass (also called its centroid) and the number of points it contains (also called its weight). This is encapsulated in the following Python classes:

{% highlight python %}
from typing import List

class Cluster:
  def __init__(self, values: List[float]):
    self._values = values

  def summarise(self) -> ClusterSummary:
    n_points = len(self._values)
    center_of_mass = sum(self._values) / n_points
    
    return ClusterSummary(center_of_mass, n_points)

class ClusterSummary:
  def __init__(self, center_of_mass: float, n_points: int):
    self._center_of_mass = center_of_mass
    self._n_points = n_points

  @property
  def centroid(self):
    return self._center_of_mass

  @property
  def weight(self):
    return self._n_points

{% endhighlight %}

To understand how the compression is done, it is easiest to consider the case when all of the data is available in memory. Below are three possible 1d clusterings of 25 data points. 

<figure>
    <a href="/assets/images/t-digest/strongly-ordered-clusters.png"><img src="/assets/images/t-digest/strongly-ordered-clusters.png"></a>
    <figcaption>Figure 1</figcaption>
</figure>

The clusters in Figure 1 have a property that the authors of the [_t_-digest paper](https://arxiv.org/abs/1902.04023) refer to as _strongly ordered_. A collection of clusters is called strongly ordered if a cluster with a smaller center of mass than another is guaranteed to have all of its points be less than that of the cluster with the larger center of mass. 

This strong ordering property can be observed visually in Figure 1. Note that all of the data points belonging to the blue cluster are less than all of the data points belonging to the green cluster, the points in the green cluster are all less than those in the yellow cluster and so on. In contrast, the clusters in Figure 2 do not exhibit strong ordering. Even though the centroid of the yellow cluster is greater than that of the green cluster, there are points belonging to the yellow cluster that are less than some of the points in the green cluster.

<figure>
    <a href="/assets/images/t-digest/weakly-ordered-cluster.png"><img src="/assets/images/t-digest/weakly-ordered-cluster.png"></a>
    <figcaption>Figure 2</figcaption>
</figure>

One of the main contributions of the paper is a systematic way of choosing the boundaries of the clusters. The _t_-digest uses what the authors call a _scale function_ which maps a percentile to a real number, which they denote by $$k$$. This function is used to define a criteria for how many points should be allowed in each cluster (more on this later). 

When a cluster is created, the percentile of the first data point in the cluster is mapped to a $$k$$ value and saved. Points are continually added to the cluster (in sorted order) until enough points are added such that the $$k$$ value has increased by 1. At this point the cluster is considered complete and the centroid as well as its weight are stored and a new cluster is started.

The cluster building process is shown in the animation in Figure 3. The crosses denote cluster centroids. Note how there are fewer points in the clusters at the extreme quantiles where the scale function is steeper.

<figure>
    <a href="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"><img src="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"></a>
    <figcaption>Figure 3</figcaption>
</figure>

Below is an implementation of this algorithm in Python.

{% highlight python %}
import numpy as np

TAU = 2 * np.pi

def scale_fn(q, delta):
  return delta / TAU * np.arcsin(2 * q - 1)

def cluster_points(points):
  sorted_points = np.sort(points)
  data_clusters = [[]]
  k_lower = scale_fn(0, delta)
  percentile_increment = 1 / n_points
  centroids = []
  for j, pt in enumerate(sorted_points):
      percentile = (j + 1) * percentile_increment
      k_upper = scale_fn(percentile, delta)

      if k_upper - k_lower < 1:
          data_clusters[-1].append(pt)
      else:
          centroids.append(np.mean(data_clusters[-1]))
          data_clusters.append([pt])
          k_lower = k_upper

  return data_clusters
{% endhighlight %}

## Scale Functions
At first glance, the choice of scale function seems complicated and arbitrary. 

The first property of a valid scale function is that it must be defined over the domain $$[0,1]$$. This ensures every percentile is mapped to a $$k$$ value. However, the main property of the scale function is that it is monotone increasing. Without this property, it is possible that the change in $$k$$ would never exceed 1, leading to no clustering at all.

Perhaps the more fundamental object than the scale function is its derivative. The slope of the scale function determines how fast the $$k$$ value increases with each percentile. This effectively sets the number of points that can be added to a cluster before it is complete. The larger the slope, the fewer points will be allowed in the cluster. The smaller the slope, the more points will be included before a new cluster is started. 

To make this notion more precise, consider the linearisation of the scale function $$k(q)$$

$$\Delta k \approx \frac{\mathrm{d}k}{\mathrm{d}q}\Delta q$$

The condition for finishing a cluster, $$\Delta k = 1$$, can be rewritten as $$\frac{\mathrm{d}q}{\mathrm{d}k} = \Delta q$$. Since $$\Delta q$$ is just the number of points in the cluster divided by the total number of data points, it can be said that the cluster sizes are proportional to $$\frac{\mathrm{d}q}{\mathrm{d}k}$$ (at least approximately). For the inverse sine scale function in the previous section, this derivative is $$\sqrt{q(1-q)}$$. This means the extreme percentiles (near 0 or 1) will be approximated by smaller, more accurate clusters (see Figure 3)

The paper goes on to suggest four scale functions with various desirable properties

$$
\begin{align*}
k_0(q) &= \frac{\delta}{2} q\\
k_1(q) &= \frac{\delta}{2\pi} \sin^{-1}(2q-1)\\
k_2(q) &= \frac{\delta}{4 \log(n/\delta) + 24} \log\left(\frac{q}{1-q}\right)\\
k_3(q) &= \frac{\delta}{4 \log(n/\delta) + 21} \cdot \left\{
     \begin{array}{lr}
       \log(2q) & q \le 0.5\\
       -2\log(2(1-q)) & q > 0.5\\
     \end{array}
   \right.
\end{align*}
$$

Each scale function has a scale parameter $$\delta$$ which determines the number of cluster centroids used to approximate the CDF. Larger values of $$\delta$$ equate to steeper scale functions. Since a steep scale function means few points will be allowed in a cluster, this translates to having many clusters. A small $$\delta$$ of course means the CDF will be approximated with fewer clusters.

<figure>
    <a href="/assets/images/t-digest/scale-functions.png"><img src="/assets/images/t-digest/scale-functions.png"></a>
    <figcaption>Figure 4</figcaption>
</figure>

# Defining the _t_-digest
## Merging digests
The previous section assumes that the data fits into memory (and there is enough memory to sort the data as well). In the scenario where each data file fits into memory, we can compute a _t_-digest per file. To get a single _t_-digest for the entire dataset, we can create a list of all the clusters from each _t_-digest and sort them by their centroids. The clusters are then merged in a greedy fashion by determining if the weight of the merged cluster (defined as the sum of the points in each cluster) leads to an increase in the $$k$$ value less than 1. If so, the two clusters can be replaced with a single merged one using the following code

{% highlight python %}
def merge_clusters(c1, c2):

  w1 = c1.weight / (c1.weight + c2.weight)
  w2 = c2.weight / (c1.weight + c2.weight)

  new_centroid = w1 * c1.centroid + w2 * c2.centroid
  new_weight = c1.weight + c2.weight

  return Cluster(new_centroid, new_weight)
{% endhighlight %}

For the most part, this algorithm is the same as the in-memory case where instead of adding points to a cluster, clusters are added to larger clusters. There is one important distinction however. 

The clusters in-memory were guaranteed to be strongly ordered, by construction. That is, a cluster with a centroid to the left of another cluster's centroid must have every data point less than that of the other cluster. After merging centroids, this property no longer holds.  The consequence of this is error in the estimation of the percentiles.

## Percentile Estimation
With the _t_-digest defined as well as methods for combining existing digests, the natural question is how to estimate an arbitrary percentile. In keeping only the centroids, along with the number of points, it is clear there is no way to get an exact answer to this question.

The assumption made for the purposes of computing percentiles from a _t_-digest is that the centroid is not just the mean of the data in the cluster but the _median_ as well. Stated another way, the assumption is that if there are $$N$$ data points in a cluster, $$N/2$$ will be to the left of the centroid and $$N/2$$ will be to the right. It is easy to find examples where this assumption is clearly false. In practice, the $$\delta$$ parameter can be tuned to create finer grained clusters so as to avoid badly skewed clusters. 

<figure>
    <a href="/assets/images/t-digest/approximate-cdf.png"><img src="/assets/images/t-digest/approximate-cdf.png"></a>
    <figcaption>Figure 5 </figcaption>
</figure>

{% highlight python %}
def cdf(td, x):
    total_weight = sum(c.weight for c in td.clusters)

    prev_percentile = 0
    percentile = 0
    prev_weight = 0
    for i, cluster in enumerate(td.clusters):
      if cluster.weight == 1:
        percentile += 1 / total_weight
        prev_weight = 0
      else:
        percentile += (cluster.weight / 2 + prev_weight / 2) / total_weight
        prev_weight = cluster.weight

      if x < cluster.centroid:
        if i == 0:
          return 0.
        delta_x = abs(cluster.centroid - prev_cluster.centroid)
        delta_y = abs(percentile - prev_percentile)
        m = delta_y / delta_x

        return prev_percentile + m * abs(x - prev_cluster.centroid)

      prev_percentile = percentile
      prev_cluster = cluster

    return 1.
{% endhighlight %}

# Conclusion
The _t_-digest is essentially a compressed representation of the data's cumulative distribution function (CDF). 

## Footnotes
<a name="footnote1">1</a>: A function is convex if $$f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$$.  

## References
1. [t-digest Paper on Arxiv](https://arxiv.org/abs/1902.04023)
2. [Intuitive Explanation of t-digest](https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest)
