---
title: "Approximate Percentiles with _t_-digests"
categories:
  - Mathematics
date:   2021-05-21 22:19:00 +0100
mathjax: true
tags:
  - Algorithms
toc: true
# classes: wide
excerpt: ""
header: 
  overlay_image: assets/images/shakespeare-zipf-param-surface-splash.png
  overlay_filter: 0.2
---

Consider the problem of computing summary statistics on a large data set. The data is so large that loading it all into memory is completely infeasible. But the data has the nice property that it has been sharded across many files, each of which do fit in memory. 

It is not difficult to work out an algorithm for computing some statistics across all of the data. For example, the maximum is one such statistic. The simplest method is to initialise the maximum to negative infinity and then read a single file into memory and compute its maximum. If it is greater than the current maximum, update it. Repeat until all the files have been read.

But suppose instead you are interested in knowing the 95th percentile of the data. In general, knowing the 95th percentile of the data in each shard of the data is insufficient to know the 95th percentile of the data globally. The _t_-digest is a probablistic data structure that allows accurate estimation of arbitrary percentiles of the _global_ data without keeping all of the data in memory.

## Summarising the Cumulative Distribution Function
The _t_-digest is essentially a compressed representation of the data's cumulative distribution function (CDF). To understand how the compression is done, it is easiest to consider the case when all of the data is available in memory. Below are three possible 1d clusterings of 25 data points. 

<figure>
    <a href="/assets/images/t-digest/random-clusters.png"><img src="/assets/images/t-digest/random-clusters.png"></a>
    <figcaption>Figure 1</figcaption>
</figure>

The clusters in Figure 1 have a property that the authors of the [_t_-digest paper](https://arxiv.org/abs/1902.04023) refer to as "strongly ordered". A collection of clusters is called strongly ordered if a cluster with a smaller center of mass (alternatively called a centroid) than another is guaranteed to have all of its points be less than that of the cluster with the larger center of mass. 

This strong ordering can be observed in Figure 1 visually as all of the data points belonging to the blue cluster being less than all of the data points belonging to the green cluster and the points in the green cluster all being less than those in the yellow cluster. In contrast, had each data point been randomly assigned to a cluster, regardless of its value, the clustering would almost certainly not exhibit strong ordering.

The clusters stored in the _t_-digest are not built by random assignment, however. Instead, a _t_-digest uses what the authors call a _scale function_ which maps a percentile to a real number, which they denote by $$k$$. This function is used to define a criteria for how many points should be allowed in each cluster. 

When a cluster is created, the percentile of the first data point in the cluster is mapped to a $$k$$ value and saved. Points are continually added to the cluster (in sorted order) until enough points are added such that the $$k$$ value has increased by 1. At this point the cluster is considered complete and the centroid (the mean of the data in the cluster), as well as the weight (the number of data points in the cluster) are stored and a new cluster is started. Below is the signature for a cluster in Python.

{% highlight python %}
class Cluster:
  def __init__(self, mean, n_points):
    self._mean = mean
    self._n_points = n_points

  @property
  def centroid(self):
    return self._mean

  @property
  def weight(self):
    return self._n_points

{% endhighlight %}

The cluster building process is shown visually in Figure 2 with the crosses denoting cluster centroids. Note how there are fewer points in the clusters at the extreme quantiles where the scale function is steeper.

<figure>
    <a href="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"><img src="/assets/gifs/t-digest/cluster-building-with-scale-function.gif"></a>
    <figcaption>Figure 2</figcaption>
</figure>

The algorithm outlined above can be written succinctly in Python as shown in the following snippet used to generate Figure 2. 

{% highlight python %}
import numpy as np

TAU = 2 * np.pi

def scale_fn(q, delta):
  return delta / TAU * np.arcsin(2 * q - 1)

def cluster_points(points):
  sorted_points = np.sort(points)
  data_clusters = [[]]
  k_lower = scale_fn(0, delta)
  percentile_increment = 1 / n_points
  centroids = []
  for j, pt in enumerate(sorted_points):
      percentile = (j + 1) * percentile_increment
      k_upper = scale_fn(percentile, delta)

      if k_upper - k_lower < 1:
          data_clusters[-1].append(pt)
      else:
          centroids.append(np.mean(data_clusters[-1]))
          data_clusters.append([pt])
          k_lower = k_upper
{% endhighlight %}

# More on Scale Functions
At first glance, the choice of scale function seems complicated and arbitrary.

- Mention the scale function must be defined on 0-1
- Preferable to have larger slope at beginnings and ends to accurately estimate extreme quantiles
- Mention dq/dk of k_1 and k_2. Explain how this derivative gives the sizes of the cluster roughly

The main property of the scale function is that it is monotone increasing. The slope of the scale function over the domain $$[0,1]$$ determines how many points need to be added to a cluster before it is complete. The steeper the slope, the fewer points will be allowed in the cluster since the $$k$$ value increases rapidly for each percentile. The flatter the slope, the more points will be included before a new cluster is started. The four scale functions defined in the paper are

$$
\begin{align*}
k_0(q) &= \frac{\delta}{2} q\\
k_1(q) &= \frac{\delta}{2\pi} \sin^{-1}(2q-1)\\
k_2(q) &= \frac{\delta}{4 \log(n/\delta) + 24} \log\left(\frac{q}{1-q}\right)\\
k_3(q) &= \frac{\delta}{4 \log(n/\delta) + 21} \cdot \left\{
     \begin{array}{lr}
       \log(2q) & q \le 0.5\\
       -2\log(2(1-q)) & q > 0.5\\
     \end{array}
   \right.
\end{align*}
$$

Each scale function has a scale parameter $$\delta$$ which determines the number of cluster centroids used to approximate the CDF. Larger values of $$\delta$$ equate to steeper scale functions. Since a steep scale function means few points will be allowed in a cluster, this translates to having many clusters. A small $$\delta$$ of course means the CDF will be approximated with fewer clusters.

<figure>
    <a href="/assets/images/t-digest/scale-functions.png"><img src="/assets/images/t-digest/scale-functions.png"></a>
    <figcaption>Figure 3</figcaption>
</figure>

The cluster size is determined by $$\Delta k$$ which, using linearisation, can be written as 

$$\Delta k \approx \frac{\mathrm{d}k}{\mathrm{d}q}\Delta q$$

The condition for finishing a cluster, $$\Delta k = 1$$, can be rewritten as $$\frac{\mathrm{d}q}{\mathrm{d}k} = \Delta q$$. And since $$\Delta q$$ is just the number of points in the cluster divided by the total number of data points, it can be said that the cluster sizes are proportional to $$\frac{\mathrm{d}q}{\mathrm{d}k}$$ (at least approximately). For $$k_1$$ this is $$\sqrt{q(1-q)}$$ whereas for $$k_2$$
the cluster sizes are proportional to $$q(1-q)$$ which has the effect of making smaller, more accurate clusters, in the tails.

# Merging _t_-digests
The previous section assumes that the data fits into memory (and there is enough memory to sort the data as well). In the scenario where each data file fits into memory, we can compute a _t_-digest per file. To get a single _t_-digest for the entire dataset, we can create a list of all the clusters from each _t_-digest and sort them by their centroids. The clusters are then merged in a greedy fashion by determining if the weight of the merged cluster (defined as the sum of the points in each cluster) leads to an increase in the $$k$$ value less than 1. If so, the two clusters can be replaced with a single merged one using the following code

{% highlight python %}
def merge_clusters(c1, c2):

  w1 = c1.weight / (c1.weight + c2.weight)
  w2 = c2.weight / (c1.weight + c2.weight)

  new_centroid = w1 * c1.centroid + w2 * c2.centroid
  new_weight = c1.weight + c2.weight

  return Cluster(new_centroid, new_weight)
{% endhighlight %}

For the most part, this algorithm is the same as the in-memory case where instead of adding points to a cluster, clusters are added to larger clusters. There is one important distinction however. 

The clusters in-memory were guaranteed to be strongly ordered, by construction. That is, a cluster with a centroid to the left of another cluster's centroid must have every data point less than that of the other cluster. After merging centroids, this property no longer holds and the clusters are instead "weakly ordered". Figure 4 shows an example of a weakly ordered cluster. Even though the green centroid is less than the yellow centroid, there are some yellow points less than green ones. The consequence of this is error in the estimation of the percentiles.

<figure>
    <a href="/assets/images/t-digest/weakly-ordered-cluster.png"><img src="/assets/images/t-digest/weakly-ordered-cluster.png"></a>
    <figcaption>Figure 4</figcaption>
</figure>

## Percentile Estimation
With the _t_-digest defined as well as methods for combining existing digests, the natural question is how to estimate an arbitrary percentile. In keeping only the centroids, along with the number of points, it is clear there is no way to get an exact answer to this question.

The assumption made for the purposes of computing percentiles from a _t_-digest is that the centroid is not just the mean of the data in the cluster but the _median_ as well. Stated another way, the assumption is that if there are $$N$$ data points in a cluster, $$N/2$$ will be to the left of the centroid and $$N/2$$ will be to the right. It is easy to find examples where this assumption is clearly false. In practice, the $$\delta$$ parameter can be tuned to create finer grained clusters so as to avoid badly skewed clusters. 

<figure>
    <a href="/assets/images/t-digest/approximate-cdf.png"><img src="/assets/images/t-digest/approximate-cdf.png"></a>
    <figcaption>Figure 5 </figcaption>
</figure>

{% highlight python %}
def cdf(td, x):
    total_weight = sum(c.weight for c in td.clusters)

    prev_percentile = 0
    percentile = 0
    prev_weight = 0
    for i, cluster in enumerate(td.clusters):
      if cluster.weight == 1:
        percentile += 1 / total_weight
        prev_weight = 0
      else:
        percentile += (cluster.weight / 2 + prev_weight / 2) / total_weight
        prev_weight = cluster.weight

      if x < cluster.centroid:
        if i == 0:
          return 0.
        delta_x = abs(cluster.centroid - prev_cluster.centroid)
        delta_y = abs(percentile - prev_percentile)
        m = delta_y / delta_x

        return prev_percentile + m * abs(x - prev_cluster.centroid)

      prev_percentile = percentile
      prev_cluster = cluster

    return 1.
{% endhighlight %}

## Conclusion
The _t_-digest data structure provides accurate quantile estimates by maintaining a list of clusters, each of which is defined by the number of points it contains and the average value of those points. Linear interpolation between the cluster centroids allows approximation of arbitrary percentiles.

## Footnotes
<a name="footnote1">1</a>: A function is convex if $$f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)$$.  

## References
1. [t-digest Paper on Arxiv](https://arxiv.org/abs/1902.04023)
2. [Intuitive Explanation of t-digest](https://dataorigami.net/blogs/napkin-folding/19055451-percentile-and-quantile-estimation-of-big-data-the-t-digest)
