---
title: "BKM"
categories:
  - Algorithms
date:   2025-04-01 12:30:00 +0100
mathjax: true
tags:
  - Numerical computing
toc: true
# classes: wide
excerpt: ""
header: 
  overlay_image: assets/bkm/images/splash_image.png
  overlay_filter: 0.2
---

In this article we'll explore yet another method for accurately calculating exponentials.
Similar in spirit to [CORDIC](../cordic), the Bajard-Kla-Muller (BKM) algorithm is a simple procedure that is friendly to hardware without floating point units (FPUs) as it involves only additions, multiplications, and bit-shifts.

- explain basic intuition of the algo as trying to build up the number
- show worked example for computing logarithm of pi
- explain how the method only works for numbers in a certain range
- explain how BKM can be used to compute the exponential

## Logarithmic BKM

We'll start with computing logarithms using the BKM algorithm in so-called _L_-mode.
The basic idea is to represent the number we want to compute the logarithm of as the product of other numbers which we _do_ know the logarithm of.

$$
x = \prod_{k=0}^n a_k
$$

Using the property $$\log(ab) = \log(a) + \log(b)$$, we can compute the logarithm as

$$
\log x = \sum_{k=1}^n \log a_k
$$

At first glance, this seems to have made the problem worse.
Instead of computing the logarithm of a single number $$x$$, we now need to compute the logarithm of $$n$$ different numbers.

But the key idea behind the algorithm is that the set of unique values in the sequence will be drawn from a small handful of numbers. This allows us to compute values of $$\log a_k$$ offline (using some other method) and then look them up in a table as needed.

### Worked Example

To see how this works in practice, it's best to illustrate with a concrete example.

Suppose we want to compute $$\log 3.14$$.
We start by initialising a variable $$x_{approx}=1$$.
We then multiply this by 2 to get $$x_{approx}=2$$.
Since this is below our desired value of $$x=3.14$$, we'll try increasing the value again but instead of doubling it, we'll multiply by 1.5.
This gives a new value of $$x_{approx}=3$$ which is still below the desired value.

We'll try increasing our value again, this time by a factor of 1.25 which gives $$x_{approx}=3.75$$.
Now our value is too large so we leave $$x_{approx}=3$$.
We then try multiplying by 1.125 which gives $$x_{approx} = 3.375$$.
This is still larger than our desired value of 3.14 so we skip this update as well.
Next we try multiplying by 1.0625 which gives $$x_{approx} = 3.1875$$, which is again too large so we skip the update.

We try multiplying $$x_{approx}=3$$ by 1.03125 which gives 3.09375.
This is less than our target value so we accept the update.
The procedure continues this way either for a finite number of iterations or until $$x_{approx}$$ is within a desired tolerance of the target.

The approximation can be concisely written as

$$
x = \prod_{k=0}^N (1+2^{-k})^{d_k}
$$

where $$d_k$$ depends on the particular input $$x$$.
Specifically, $$d_k$$ is 0 if $$a_k$$ is needed in the approximation of $$x$$ and 0 otherwise.
Given this equivalent expression for $$x$$, $$\log x$$ can be computed as

$$
\log x = \sum_{k=0}^N d_k \log(1+2^{-k})
$$

Figure 1 shows the steps in building up $$x_{approx}$$ for $$x=3.14$$.

<figure class>
    <a href="/assets/bkm/images/log_pi.png"><img src="/assets/bkm/images/log_pi.png"></a>
    <figcaption>Figure 1: Approximation of 3.14 using factors of the form, one plus reciprocal powers of 2. The blue indicate accepted points in the approximation while red crosses are ones which were rejected for overshooting the target value 3.14</figcaption>
</figure>

### Code

The main benefit of BKM is that it can be implemented easily on hardware without floating point units.
If implemented using fixed precision arithmetic, the $$2^{-k}$$ can be implemented as a $$k$$-bit right shift so that no divisions are necessary.

The following python code implements the algorithm using floating point to avoid getting lost in the details of fixed point implementations.

{% highlight python %}
import math

LOGARITHM_LOOKUP = [math.log(1 + 2.0**-k) for k in range(100)]

def log(x: float, n_iters: int = 30):
    assert n_iters < 30

    log_x = 0
    x_hat = 1
    factor = 1
    for k in range(n_iters):
        tmp = x_hat + x_hat * factor  # x * (1 + 2**-k)
        if tmp <= x:
            log_x += LOGARITHM_LOOKUP[k]
            x_hat = tmp
        factor /= 2
    return log_x
{% endhighlight %}

Figure 2 shows the effect of choosing a larger number of iterations to approximate the logarithm.
Note how the approximation is piecewise constant (the number of levels is $$2^{n_{iters}}$$)

<figure class>
    <a href="/assets/bkm/images/bkm_lmode.png"><img src="/assets/bkm/images/bkm_lmode.png"></a>
    <figcaption>Figure 2: L-mode BKM used to approximate the logarithm using different numbers of iterations. At 8 iterations, the approximation is already quite good.</figcaption>
</figure>

If we zoom out on this approximation, we see there's actually a problem.

<figure class>
    <a href="/assets/bkm/images/bkm_log_artifacts.png"><img src="/assets/bkm/images/bkm_log_artifacts.png"></a>
    <figcaption>Figure 3: For values less than 1 or greater than ~4.768, we no longer get accurate approximations of the logarithm.</figcaption>
</figure>

This issue does not go away simply by choosing $$n_{iters}$$ to be larger but rather is a fundamental limitation of BKM.
By approximating our argument as $$x =\prod_{k=0}^N (1+2^{-k})^{d_k}$$, the maximum value we can achieve (i.e. all $$d_k=1$$) is ~4.768 as shown in figure 4.

<figure class>
    <a href="/assets/bkm/images/cumulative_product.png"><img src="/assets/bkm/images/cumulative_product.png"></a>
    <figcaption>Figure 4: The product approximation asymptotes around 4.768.</figcaption>
</figure>

Similarly, the smallest value we can achieve (i.e. all $$d_k = 0$$) is 1.
So before using BKM, we have to ensure our argument lies in this interval.
As an example, suppose we wanted to compute $$\log 6.28$$.
We could rewrite this as $$\log(2 \cdot 3.14) = \log 2 + \log 3.14$$ and use BKM to compute each of the logarithms.
This process is called _argument reduction_.

## Exponential BKM

In the previous section we discussed how BKM is used to compute the natural logarithm.
Using a very similar procedure, it can be adapted to compute the exponential as well (a.k.a E-mode BKM).

<figure class>
    <a href="/assets/bkm/images/bkm_emode.png"><img src="/assets/bkm/images/bkm_emode.png"></a>
    <figcaption>Figure 4: The product approximation asymptotes around 4.768.</figcaption>
</figure>
<figure class>
    <a href="/assets/bkm/images/bkm_exp_artifacts.png"><img src="/assets/bkm/images/bkm_exp_artifacts.png"></a>
    <figcaption>Figure 4: The product approximation asymptotes around 4.768.</figcaption>
</figure>
<!-- <figure class="half">
    <a href="/assets/cordic/images/circular_angle_01.png"><img src="/assets/cordic/images/circular_angle_01.png"></a>
    <a href="/assets/cordic/images/circular_angle_02.png"><img src="/assets/cordic/images/circular_angle_02.png"></a>
    <figcaption>Figure 8: Area of sectors for different theta.</figcaption>
</figure> -->

## Conclusion

## Footnotes

<a name="footnote1">1</a>: The determinant of the matrix is $$\cosh^2 \theta - \sinh^2 \theta = 1$$ meaning that areas are preserved. The matrix is _not_ orthogonal however so lengths of vectors are not preserved by the transformation.
